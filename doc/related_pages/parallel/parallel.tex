%%%%%%%%%%%%%%%%%%%%%definitions%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{../header.tex}
\input{../newcommands.tex}
%\newcommand{\RR}{{R'}}
%\newcommand{\ZZ}{{Z'}}
%\newcommand{\PP}{{\varphi'}}
\newcommand{\RR}{{\rho}}
\newcommand{\ZZ}{{\zeta}}
\newcommand{\PP}{{\Phi}}
\newcommand{\CC}{s}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%DOCUMENT%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
%\preprint{}

\title{The parallel derivative on structured grids}
\author{M.~Wiesenberger and M.~ Held}

\maketitle

\abstract{
This write-up shows how we numerically treat parallel derivatives in a non field
aligned coordinate system. It is mainly based on References~\cite{Hariri2014,Held2016,Stegmeir2017} and contains parts from References~\cite{WiesenbergerPhD, HeldPhD}
}
\tableofcontents
\section{Discretization of parallel derivatives} \label{sec:parallel}
We introduce the method
and discuss some problems arising from the boundaries of the computational domain.

\subsection{The flux coordinate independent approach} \label{sec:parallela}
Given is a vector field $\vec v(R,Z)$ in cylindrical coordinates $R,Z,\varphi$ independent of $\varphi$ and we want to
discretize the derivative $\vec v \cdot\nabla f \equiv \nabla_\parallel f$.
The vector field $\vec v$ might be the magnetic unit vector field but the algorithm works
for any vector field $\vec v$ with $v^\varphi\neq 0$, in particular $\vec v$ does not need
to have unit length.

We begin with the formulation of a field-aligned discretization.
To every smooth vector field $\vec v(\vec x)$ there is a unique curve of which the
tangent in a point $p$ is the value of $\vec v(p)$ at that point. It is given by
the solution of the differential equation
\begin{align}
  \frac{\d z^i}{\d s} = v^i(\vec z)|_{\vec z(s)}
    \label{eq:integralcurve}
\end{align}
where $z^i$ is one of $(R, Z, \varphi)$ and $v^i$ are the contravariant components
of $\vec v$ in cylindrical coordinates.
\begin{tcolorbox}[title=Note]
Note here that $s$ does NOT necessarily denote the distance
(especially since we do not require the existence of a metric at this point).
\end{tcolorbox}
Moreover, by definition we have
\begin{align}
    \frac{\d f(\vec z(s))}{\d s} = \vec v\cdot \nabla f|_{\vec z(s)}
    \label{eq:fieldline_original}
\end{align}
along a field line parameterized by $s$.
This means that instead of $\vec v \cdot \nabla f$ we can choose to discretize ${\d f}/{\d s}$.

Let us divide the $\varphi$ direction into $N_\varphi$ equidistant planes of
$\Delta \varphi$. Unfortunately, from Eq.~\eqref{eq:integralcurve} we cannot easily determine
$\Delta s$ for given $\Delta \varphi$.
Thus, it is better to reparameterize Eq.~\eqref{eq:fieldline_original} with $\varphi$ instead of $s$:
\begin{subequations}
\begin{align}
    \frac{\d R}{\d\varphi}&= \frac{v^R}{v^\varphi},\\ %\frac{R}{I}\frac{\partial\psi}{\partial Z},\\
    \frac{\d Z}{\d\varphi}&=\frac{v^Z}{v^\varphi},\\%-\frac{R}{I}\frac{\partial\psi}{\partial R}.
    \frac{\d s}{\d\varphi}&=\frac{1}{v^\varphi}
\end{align}
\label{eq:fieldline}
\end{subequations}
We integrate Eqs.~\eqref{eq:fieldline} from $\varphi=\varphi_k$ to $\varphi=\varphi_k\pm \Delta \varphi$
with initial condition
\begin{align}
    (R(\varphi_k), Z(\varphi_k), s(\varphi_k) ) = (R, Z, s_k).
    \label{}
\end{align}
where $k$ is the numbering of the planes and $s_k$ is arbitrary.
Let us characterize the solution $(R(\varphi_k\pm \Delta \varphi), Z(\varphi_k\pm \Delta \varphi))$ to Eqs.~\eqref{eq:fieldline} as the flow generated by $\vec v/v^\varphi$
\begin{align}
    \Tpm\vec z \equiv \Tpm[R, Z, \varphi]:= ( R(\pm \Delta\varphi), Z( \pm \Delta\varphi), \varphi\pm\Delta \varphi),
    \label{}
\end{align}
Obviously we have $\Tm\circ\Tp = \Eins$ (and this is in general well fulfilled
numerically ), but $\Tpm$ is not necessarily unitary since $\vec v/v^\varphi$ is in general
not divergence free.

We now fit a second order polynomial through $(R,Z,\varphi)$, $\Tp [R,Z,\varphi]$ and $\Tm[R,Z,\varphi]$. We approximate the first and second derivative at $(R,Z,\varphi)$ by evaluating the derivative of the
polynomial at that point.
\begin{align} \label{eq:paralleldis}
    \nabla_\parallel f \equiv \frac{df}{ds}
    \rightarrow
    \left(\frac{ 1}{s_{k+1}-s_{k-1}} - \frac{1}{s_k - s_{k-1}}\right) &f_{k-1}  \nonumber\\
    +\left(\frac{ 1}{s_{k}-s_{k-1}} - \frac{1}{s_{k+1} - s_{k}}\right) &f_k\nonumber\\
    +\left(\frac{ 1}{s_{k+1}-s_{k}} - \frac{1}{s_{k+1} - s_{k-1}}\right) &f_{k+1}
    %\frac{f\left(T_{\Delta\varphi}^+\vec z_k\right)-f\left(T_{\Delta\varphi}^-\vec z_k\right)}{s_{k+1}-s_{k-1}},
\end{align}
where $\vec z_k = (R,Z,\varphi_k)$ and $f_k = f(\vec z_k)$
and $f_{k\pm 1} := f(\Tpm \vec z_k)$.
\begin{tcolorbox}[title=Note]
Eq.~\eqref{eq:paralleldis} reduces to
the familiar centered difference formula in case of equidistant spacings.
\end{tcolorbox}
Eq.~\eqref{eq:paralleldis} is slightly different from Reference~\cite{Hariri2014}, where the $\d f/\d\varphi v^\varphi$ is discretized to avoid integrating $s$.
However, our discretization has the advantage that it can be used for higher
order derivatives as well (e.g. $\d^2f/\d \varphi^2$), which is not possible otherwise due to the chain rule
for $v^\varphi$.

Since the $R$ and $Z$ coordinates are still discretized in the dG framework we note that in our work
the interpolation of $f$ on the transformed points $\Tpm\vec z$
is naturally given by interpolating the base polynomials.
Let us for a moment omit the $Z$ coordinate for ease of notation.
If $(R_{nj}, \varphi_k)$ are the grid points,
we call $(R^+_{nj}, \varphi_{k+1}) := \Tp[R_{nj}, \varphi_k]$ and
$(R_{nj}^-, \varphi_{k-1}) := \Tm[R_{nj}, \varphi_k]$ the transformed coordinates along
the field lines. We then have
\begin{subequations}
\begin{align}
    f(\Tp\vec z) = f( R^+_{nj}, \varphi_{k+1}) = \bar f_{k+1}^{ml}p_{ml}(R^+_{nj}) =: (I^+)_{nj}^{ml}f_{(k+1)ml} , \\
    f(\Tm\vec z) = f( R^-_{nj}, \varphi_{k-1}) = \bar f_{k-1}^{ml}p_{ml}(R^-_{nj}) =: (I^-)_{nj}^{ml}f_{(k-1)ml} , 
\end{align}
\label{eq:interpolation}
\end{subequations}
where the backward transformations of $\bar{ \vec f}$ are hidden in $I$.
Thus, the interpolation of all the necessary points can simply be written as a matrix-vector product, where the interpolation matrices $I^+$  and $I^-$ are independent of time since
the field lines are constant in time. The order of this interpolation is given by $P$, the number of polynomial coefficients.
A consistency check is the relation $I^+\circ I^- = \Eins$.

The discretization~\eqref{eq:paralleldis} can now be written as a matrix vector product
\begin{align}
\nabla_\parallel f \rightarrow  \left[S^+ \circ \Eins^+\otimes I^+ + S^0  + S^-\circ \Eins^- \otimes I^-  \right] \vec f,
    \label{}
\end{align}
where $S^+$, $S^0$ and $S^-$ are the diagonal matrices that contain the prefactors
in Eq.~\eqref{eq:paralleldis}.
This discretization is not skew-symmetric since the
field lines are not volume-preserving, or~$(I^+)^\mathrm{T} \neq I^-$.
In fact, the adjoint of the parallel derivative is
\begin{align}
    \nabla_\parallel^\dagger f = - \nabla\cdot(\vec v\ f ) \neq -\nabla_\parallel f.
    \label{}
\end{align}
Note that with this relation we can define the parallel
diffusion operator as
\begin{align}
    \Delta_\parallel := -\nabla_\parallel^\dagger \nabla_\parallel = (\nabla\cdot \vec{ \hat v}) \nabla_\parallel + \nabla_\parallel^2 , 
    \label{}
\end{align}
which is indeed the parallel part of the full Laplacian $\Delta = \nabla\cdot( \vec{ \hat v} \nabla_\parallel + \nabla_\perp)$.
$\vec{ \hat v} $ is the unit vector $\vec v/ |\vec v|$.

Note that the second order derivative $\nabla_\parallel^2$ can be
discretized using
\begin{align}\label{eq:second_order}
    \frac{\d^2 f}{\d s^2} \rightarrow
     \frac{2f_{k+1}}{(s_{k+1}-s_k)(s_{k+1}-s_{k-1})}
    -\frac{2f_{k}}{(s_{k+1}-s_k)(s_{k}-s_{k-1})} \nonumber\\
    +\frac{2f_{k-1}}{(s_{k}-s_{k-1})(s_{k+1}-s_{k-1})}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Change of coordinates}
In principle the above considerations hold in any
coordinate system $\eta,\zeta,\varphi$, since the directional derivative is
an intrinsic operation.
The only question is how to integrate the field lines in the
$\eta, \zeta,\varphi$ system
since we assumed that our vector field $\vec v(\vec x)$ was given
analytically in
cylindrical coordinates. There are two possibilities.
First, interpolate $R(\zeta_i, \eta_i), Z(\zeta_i, \eta_i)$ for
all $i$, then integrate $\vec v$ in $(R,Z)$ space and finally use
Newton iteration to find $\zeta(R^\pm_i, Z^\pm_i), \eta(R^\pm_i, Z^\pm_i)$.
The downside here is that it is difficult to tell when and where the fieldline leaves the simulation domain. (However this is true also for the
following approach, See Section~\ref{sec:boundary})

The second possibility (the one currently implemented)
is to integrate entirely in the
transformed coordinate system $\zeta, \eta, \varphi$.
The magnetic field can be easily transformed since we have the
Jacobian of the coordinate transformation
\begin{align}
    v^\zeta(\zeta, \eta) &= \left(\frac{\partial \zeta}{\partial R} v^{R} + \frac{\partial \zeta}{\partial Z}v^Z\right)_{R(\zeta, \eta), Z(\zeta, \eta)} \\
    v^\eta(\zeta, \eta) &= \left(\frac{\partial \eta}{\partial R} v^{R} + \frac{\partial \eta}{\partial Z}v^Z\right)_{R(\zeta, \eta), Z(\zeta, \eta)} \\
    v^\varphi(\zeta, \eta) &= v^\varphi({R(\zeta, \eta), Z(\zeta, \eta)})
    \label{eq:field_trafo}
\end{align}
The fieldline equations~\eqref{eq:fieldline} are still
\begin{subequations}
\begin{align}
\frac{\d \zeta}{\d\varphi} &= \frac{v^\zeta}{v^\varphi}\\
\frac{\d \eta}{\d\varphi} &= \frac{v^\eta}{v^\varphi}\\
\frac{\d s}{\d \varphi} &= \frac{1}{v^\varphi}
\end{align}
\label{eq:fieldlines_coords}
\end{subequations}
The issue here is that when integrating fieldlines we
have to interpolate the vector field $\vec v$ at arbitrary points
instead of simply evaluating the exact values.
However, the interpolation error vanishes with order $P$ in the
perpendicular plane so in order to mitigate this error
we transform $\vec v$ on a finer grid/higher order polynomials for more accurate
integration.
Apart from the issue of how to get the transformed vector field
the remaining algorithm for $\vec v\cdot\nabla$ is entirely unchanged
and Eq.~\eqref{eq:paralleldis} still holds.

\subsection{Boundary conditions} \label{sec:boundary}
The question is what to do when a fieldline intersects with the boundary
of the simulation domain before reaching the next plane.
Boundary conditions are formulated by either setting a value
on the boundary of the domain (Dirichlet) or by fixing
the derivative perpendicularly to the boundary (Neumann), or
a combination of both ( Robin).


\subsubsection{ Boundary conditions perpendicular to the wall}
The issue with Neumann boundary conditions is that they usually prescribe
derivatives perpendicular to the boundary
while the fieldlines are in general not perpendicular to the boundary.
One possible approach is to introduce ghostcells at the
places where fieldlines end. The value of the ghostcells are
as if we Fourier transformed the fields on the simulation domain
with the correct boundary conditions and thus have a periodic
extension of the fields beyond the boundaries.
For example, for Neumann boundary
conditions the field is effectively mirrored at the boundary, while for
Dirichlet boundary conditions the values are the negative mirror image. In one
dimension this reads
\begin{align}
f(x) = \pm f(2x_b-x)
\end{align}
where $x_b$ is the closest boundary $+$ is for Neumann, and $-$ for Dirichlet conditions.
This is very easily implemented (at least as long as we only allow
homogeneous Boundary conditions) since we don't actually have to
perform the Fourier transformation, we only need to mirror the end
coordinates at the boundary and then choose either the positive (Neumann) or
negative (Dirichlet) interpolation.

The above procedure works for cylindrical coordinates where we can
evaluate and thus integrate the vector field $\vec v$ even outside the domain.
This is unfortunately not true for transformed coordinates.
For now we have to rely on the fieldlines being aligned to the
boundary in these cases to avoid boundary conditions altogether.

The downside of this approach is that the mirrored point of a field-line
can lie very far away from the wall if the resolution in $\varphi$ is low (which
is the motivation to implement FCI in the first place) and
in addition also on a different flux surface.
In principle we need to increase the $\varphi$ resolution until we resolve
the perpendicular direction, in order to reliably converge with this method.
In practice we can often run a simulation stably with low resolution nevertheless.

In order to avoid coupling different flux surfaces through the boundary condition, we have the possibility to
mirror also the flux surfaces at the boundary.
This approach converges well and works stably if the flux surface is (or is close to) perpendicular to the wall.

\subsubsection{ Boundary conditions parallel to the fieldline}
The natural way to implement boundary conditions is to work them into the
interpolating polynomial. The boundary condition then replaces a third
point by setting
either a predefined value (Dirichlet) or derivative (Neumann) on the boundary.
Evaluating the interpolating polynomial at the middle point then yields
(assuming that $\Tp \vec z$ lies outside the boundary and $s_k <s_b^+ < s_{k+1}$ is where the boundary lies)
\begin{align} \label{eq:paralleldis_neup}
    \frac{df}{ds}
    \underset{\textsc{NEU}}{\rightarrow}
    \left(\frac{ 1}{s_{k}-s_{k-1}} - \frac{1}{2(s_b^+-s_k) + (s_k-s_{k-1})} \right) &(f_k-f_{k-1})\nonumber\\
    +\left(\frac{s_k - s_{k-1}}{2(s_b^+-s_k) + (s_k-s_{k-1})}\right) &f_{b+}'\\
\label{eq:second_order_neup}
    \frac{\d^2 f}{\d s^2}
    \underset{\textsc{NEU}}{\rightarrow}
    \frac{2}{2(s_b^+-s_k) + (s_k-s_{k-1})}\left(  f_{b+}' - \frac{1}{s_k - s_{k-1}}(f_k-f_{k-1})\right)
\end{align}
where $f_{b+}'$ is the value of the  derivative on the boundary (currently we only allow homogeneous boundary conditions, i.e. $f_{b+}' \equiv 0$).
If the boundary lies at $s_{k-1}<s_b^-<s_k$ then we have
\begin{align} \label{eq:paralleldis_neum}
    \frac{df}{ds}
    \underset{\textsc{NEU}}{\rightarrow}
    \left(\frac{ 1}{s_{k+1}-s_k} - \frac{1}{2(s_k - s_b^-) + (s_{k+1}-s_k)} \right) &(f_{k+1}-f_k)\nonumber\\
    +\left(\frac{s_{k+1} - s_k}{2(s_k - s_b^-) + (s_{k+1}-s_k)}\right) &f_{b-}'
    \\
\label{eq:second_order_neum}
    \frac{\d^2 f}{\d s^2}
    \underset{\textsc{NEU}}{\rightarrow}
    \frac{2}{2(s_k -s_b^-) + (s_{k+1}-s_{k})}\left(  -f_{b-}' + \frac{1}{s_{k+1}-s_k} (f_{k+1} - f_k)\right)
\end{align}
while if the fieldline intersects the wall on both ends we have
\begin{align} \label{eq:paralleldis_neupm}
    \frac{df}{ds}
    \underset{\textsc{NEU}}{\rightarrow}
    \frac{ 1}{s_b^+ - s_b^-}\left[({ s_k - s_b^-})f_{b+}' + ({s_{b}^+-s_k}) f_{b-}'\right]    \\
\label{eq:second_order_neupm}
    \frac{\d^2 f}{\d s^2}
    \underset{\textsc{NEU}}{\rightarrow}
    \frac{ 1}{s_b^+ - s_b^-}( f_{b+}' - f_{b-}')
\end{align}
The formulas for Dirichlet boundary conditions are the same as the original
formulas, with $s_{k+1}$, $f_{k+1}$ replaced by $s_b^+$, $f_b^+$ and/or $s_{k-1}$, $f_{k-1}$ replaced by $s_b^-$, $f_b^-$.

There is no difference between those formulas and actually evaluating the
interpolating polynomial at a ghost point and then using that point in the
original formulas. The reason is that the interpolating polynomial is unique
and thus is the value of its derivatives at $s_k$.

For the above formulas to work we need to find the exact place where the fieldline intersects
the boundary.
We have to find
$\varphi_b$ such that the result of the integration of Eq.~\eqref{eq:fieldline} from
$\varphi$ to $\varphi_b$ lies on the boundary.
The angle $\varphi_b$ can be found by a bisection algorithm knowing that $\varphi_k<\varphi_b < \varphi_k + \Delta\varphi$.
This kind of procedure is known as a shooting method.
Another possibility is to trick the fieldline integrator into finding the point for us.
We do this by setting $\vec v \equiv 0$ on all points outside the simulation box, which
makes the ODE integrator stop once it crosses the domain boundary.
This works fairly well with the adaptive embedded Runge Kutta method that we use
and in particular also works in transformed coordinates.

The advantage of this method is that the parallel derivative
does not couple different field lines through the boundary conditions. Thus
the dynamics on each field-line can be completely independent.
%However, the problem with this procedure in practice is the small
%distance between the starting point and the point where the corresponding
%fieldline intersects the boundary. This seriously deteriorates the
%CFL condition. (To ease the CFL condition
%was the reason to devise the algorithm in the first place)

\subsubsection{Avoiding boundary conditions in non-aligned systems} \label{sec:avoid}

When computing in non-aligned coordinate systems
one idea to avoid boundary conditions
is to simply cut the contribution from field lines
that leave the computational domain. While this might work in practice
it is \textbf{highly unclear} what numerical and physical side-effects this procedure might have.

Another solution would be to change the
vector field $\vec v$ and only retain the toroidal part of $\vec v$ on the
boundary ( $v^R|_{\partial\Omega} = v^Z|_{\partial\Omega} =0$). The fieldlines then have a kink on the boundary $\partial\Omega$.
On the other hand we can implement boundary conditions consistent with
the perpendicular ones since the fieldlines never leave the domain.
We simply interpolate the quantity to derive on the inner side of the
domain boundary (Neumann conditions = "No boundary condition") or
set the value to zero (Dirichlet condition).
\textbf{Unfortunately, when testing this procedure with an analytical solution
the error does not converge neither for Dirichlet nor for Neumann.}

\subsection{Poloidal limiters}
A poloidal limiter can simply be implemented via a boundary condition in $\varphi$.
As long as the form of the limiter is aligned with a flux-function we do not have to
integrate a field line in order to determine which points lie in the
limiter-shadow. It is therefore straightforward to implement ghost-cells
in that case.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Fieldaligned coordinate system}
\subsection{Constructing the coordinate map and the metric elements}
The idea is to construct a locally fieldaligned coordinate system that
for a given poloidal plane coincides with the existing coordinate system.
We construct coordinates by integrating the fieldline equation with
 the poloidal grid coordinates $R_0$, $Z_0$ as initial conditions.
 We call the resulting coordinates $\RR$, $\ZZ$, $\PP$.
 We parameterize the fieldline with the toroidal angle $\PP$.
 We thus have the base vectors
 \begin{align}
 \partial_\RR &= R_\RR \partial_R + Z_\RR\partial_Z \\
 \partial_\ZZ &= R_\ZZ \partial_R + Z_\ZZ\partial_Z \\
 \partial_\PP &= R_\PP \partial_R + Z_\PP\partial_Z + \varphi_\PP\partial_\varphi
 \end{align}
 By definition we have
 \begin{align}
 R_\PP &= v^R(R,Z) \\
 Z_\PP &= v^Z(R,Z) \\
 \varphi_\PP &= 1
 \end{align}
 We have the covariant base vectors
 \begin{align}
\d \RR &= \RR_R\d R  + \RR_Z \d Z + \RR_\varphi \d\varphi \\
\d \ZZ &= \ZZ_R\d R  + \ZZ_Z \d Z + \ZZ_\varphi \d\varphi \\
\d \PP &=  \d\varphi
\end{align}
Note that the elements of the forward and backward transformation are related through the
Jacobian
\begin{align}
\begin{pmatrix}
R_\RR & R_\ZZ & R_\PP \\
Z_\RR & Z_\ZZ & Z_\PP \\
\varphi_\RR & \varphi_\ZZ & \varphi_\PP
\end{pmatrix}
&=
\begin{pmatrix}
\RR_R & \RR_Z & \RR_\varphi \\
\ZZ_R & \ZZ_Z & \ZZ_\varphi \\
0 & 0 & 1
\end{pmatrix}^{-1}
\nonumber\\
&=
\begin{pmatrix}
 \ZZ_Z & -\RR_Z & (\RR_Z \ZZ_\varphi - \ZZ_Z\RR_\varphi) \\
 -\ZZ_R & \RR_R &  (\RR_\varphi \ZZ_R - \ZZ_\varphi \RR_R) \\
0 & 0 & (\RR_R \ZZ_Z - \ZZ_R \RR_Z)
\end{pmatrix}
\frac{1}{(\RR_R \ZZ_Z - \ZZ_R \RR_Z)}
\end{align}
% J is defined such that the volume transformation comes out like it does
We immediately see that the determinant of the Jacobian (relevant for the
volume form) is given by $\det J = (R_\RR Z_\ZZ - Z_\RR R_\ZZ) = 1/(\RR_R \ZZ_Z - \ZZ_R \RR_Z)$.
In order to integrate the elements of the Jacobian we employ Nemov's algorithm.
Notice first that $\d \RR/ \d\PP = \d \ZZ / \d\PP  = 0$.
In words, if we go along a streamline of $\partial_\PP$ (i.e. a fieldline) we
stay on the same value of $\RR$. (This is basically the definition of a coordinate line).
Next, we can write $\d / \d\PP = (\vec v/v^\varphi) \cdot \nabla $ and thus
$\vec v \cdot \nabla \RR = 0$. If we derive this we get
\begin{align}
\partial_R ( (\vec v/v^\varphi)\cdot\nabla \RR ) = \vec v_R \cdot \nabla\RR/v^\varphi + (\vec v/v^\varphi)\cdot \nabla \RR_R = 0
\end{align}
and analogous for the other elements. We thus find
\begin{align}
\frac{\d R}{\d\PP} = \frac{v^R}{ v^\varphi} \quad
\frac{\d Z}{\d\PP} = \frac{v^Z}{ v^\varphi} \\
\frac{\d \RR_R}{\d\PP} =
- \left(\frac{v^R}{ v^\varphi}\right)_R \RR_R
- \left(\frac{v^Z}{ v^\varphi}\right)_R \RR_Z
\qquad
\frac{\d \RR_Z}{\d\PP} =
- \left(\frac{v^R}{ v^\varphi}\right)_Z \RR_R
- \left(\frac{v^Z}{ v^\varphi}\right)_Z \RR_Z
\\
\frac{\d \ZZ_R}{\d\PP} =
- \left(\frac{v^R}{ v^\varphi}\right)_R \ZZ_R
- \left(\frac{v^Z}{ v^\varphi}\right)_R \ZZ_Z
\qquad
\frac{\d \ZZ_Z}{\d\PP} =
- \left(\frac{v^R}{ v^\varphi}\right)_Z \ZZ_R
- \left(\frac{v^Z}{ v^\varphi}\right)_Z \ZZ_Z
\end{align}
As initial conditions we use that the coordinate system is supposed to coincide with $R$, $Z$
at the origin, that is $\RR_R = 1$, $\RR_Z = 0$, $\ZZ_R = 0$, $\ZZ_Z = 1$.
From these identities we can derive further
\begin{align}
\frac{\d (\RR_R \ZZ_Z - \ZZ_R \RR_Z)}{\d\PP} =
- \left(\left(\frac{v^R}{v^\varphi}\right)_R
      + \left(\frac{v^Z}{v^\varphi}\right)_Z\right) ( \RR_R\ZZ_Z - \RR_Z\ZZ_R) \\
\frac{\d  (R_\RR Z_\ZZ - R_\ZZ Z_\RR)}{\d\PP} =
+\left(\left(\frac{v^R}{v^\varphi}\right)_R
      +\left(\frac{v^Z}{v^\varphi}\right)_Z\right) ( R_\RR Z_\ZZ - Z_\RR R_\ZZ)
\end{align}
The last identity is particularly interesting if we are only interested in the volume element in the
new coordinate system $\sqrt{G} = \det J \sqrt{g}$.
In order to obtain the volume element directly (without integrating all the
elements of the Jacobian separately) we compute
\begin{align}
\frac{\d \sqrt{G}}{\d\PP} &=
\frac{\d \sqrt{g}\det J}{\d\PP} =
\left[\frac{v^R}{v^\varphi}\sqrt{g}_R + \frac{v^Z}{v^\varphi}\sqrt{g}_Z + \left(\frac{v^R}{v^\varphi}\right)_R
      +\left(\frac{v^Z}{v^\varphi}\right)_Z\right]\det J \nonumber\\
      &= \left[\partial_R\left( \sqrt{g}\frac{v^R}{v^\varphi}\right) + \partial_Z\left(\sqrt{g}\frac{v^Z}{v^\varphi}\right) \right]\det J
          = \nc\left(\frac{\vec v}{v^\varphi}\right) \sqrt{g}\det J = \nc\left(\frac{\vec v}{v^\varphi}\right)\sqrt{G}
\label{eq:volume_integration}
\end{align}

The fieldaligned gradient, divergence and Laplacian in the fieldaligned coordinate system read
with $v^\PP(\RR,\ZZ,\PP) = v^\varphi( R(\RR,\ZZ,\PP), Z(\RR,\ZZ,\PP), \PP)$
\begin{align}
\nabla_\parallel f (\RR,\ZZ,\PP) =  \vec v \cdot\nabla f &= v^\PP\partial_\PP f \\
\nabla\cdot (\vec v f) (\RR,\ZZ,\PP) &=  \frac{1}{\sqrt{G}} \partial_\PP\left(\sqrt{G} v^\PP f \right)\label{eq:div_aligned}\\
\Delta_\parallel f (\RR,\ZZ,\PP) =  \nabla\cdot (\vec v\vec v\cdot \nabla f)  &= \frac{1}{\sqrt{G}}
    \partial_\PP\left(\sqrt{G} v^\PP v^\PP \partial_\PP f\right)
\end{align}
Note that Eq.~\eqref{eq:div_aligned} with $f=1$ is probably the easiest way to derive Eq.~\eqref{eq:volume_integration}
and a good consistency test.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Discrete operators in the fieldaligned coordinate system}
For a function $f$ given in a non-aligned coordinate system we can pull-back
$f$ into the (locally) fieldaligned coordinate system via
\begin{align}
    f_{k+1} := \Tm f( \RR,\ZZ,\PP_{k+1}) := f( \Tp \vec z) \\
    f_{k-1} := \Tp f( \RR,\ZZ,\PP_{k-1}) := f( \Tm \vec z)
\end{align}
Once transformed it is straightforward to discretize derivatives
\begin{align}
    \nabla_\parallel^{FW} &\rightarrow v^\PP_k \frac{f_{k+1} - f_k}{\Delta\PP} \label{eq:forward}\\
    \nabla_\parallel^{BW} &\rightarrow v^\PP_k \frac{f_{k} - f_{k-1}}{\Delta\PP} \label{eq:backward} \\
    \nabla_\parallel^{CC} &\rightarrow v^\PP_k \frac{f_{k+1} - f_{k-1}}{2\Delta\PP}\label{eq:centered} \\
    \nc(\vec v f)^{FW} &\rightarrow\frac{\sqrt{G^{k+1}} v^\PP_{k+1} f_{k+1} - \sqrt{G^k} v^\PP_k f_k}{\sqrt{G^k}\Delta\PP} \label{eq:divForward}\\
    \nc(\vec v f)^{BW} &\rightarrow \frac{\sqrt{G^{k}} v^\PP_{k} f_{k} - \sqrt{G^{k-1}} v^\PP_{k-1} f_{k-1}}{\sqrt{G^k}\Delta\PP}\label{eq:divBackward}\\
    \nc(\vec v f)^{CC} &\rightarrow \frac{\sqrt{G^{k+1}} v^\PP_{k+1} f_{k+1} - \sqrt{G^{k-1}} v^\PP_{k-1} f_{k-1}}{2\sqrt{G^k}\Delta\PP} \label{eq:divCentered}\\
    \Delta_\parallel &\rightarrow \frac{1}{\sqrt{G^k}\Delta\PP^2}\left[\sqrt{G^{k+1/2}} v^\PP_{k+1/2} v^\PP_{k+1/2}(f_{k+1}-f_k)
    \right.   \nonumber\\ &\left.
    - \sqrt{G^{k-1/2}}v^\PP_{k-1/2}v^\PP_{k-1/2}(f_k- f_{k-1})\right] \label{eq:laplace}
\end{align}
where we define
\begin{align}
    \sqrt{G^{k+1/2}} := 0.5\left( \sqrt{G^{k+1}} + \sqrt{G^k}\right)
    \qquad
    \sqrt{G^{k-1/2}} := 0.5\left( \sqrt{G^{k-1}} + \sqrt{G^k}\right)\\
    v^\PP_{k+1/2} := 0.5\left( v^\PP_{k+1} + v^\PP_k\right)
    \qquad
    v^\PP_{k-1/2} := 0.5\left( v^\PP_{k-1} + v^\PP_k\right)
\end{align}
Notice that only the relative quantities $\sqrt{G^{k+1}}/\sqrt{G^k}$ and $\sqrt{G^{k-1}}/\sqrt{G^k}$ need to be stored.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The adjoint methods}
\subsection{A grid refinement approach}
The idea is to discretize the operation $\nabla\cdot( \vec v .)$ by
taking the adjoint of the discretization for $\nabla_\parallel$ i.e. Eq.~\eqref{eq:paralleldis}.
Remember that the adjoint of a matrix
involves the volume element (including dG weights). This means that after you've transposed the
parallel derivative Eq.~\eqref{eq:paralleldis}, simply bracket the result
by $1/\sqrt{g}$ and $\sqrt{g}$ to get the adjoint.

While the idea of simply transposing the discretization matrices sounds appealing the problem
is that the resulting discretization does not converge.
One idea to solve this problem \cite{Stegmeir2017} is
to bracket the parallel derivative by interpolation ($Q$) and
projection ($P$) matrices:
\begin{align}
    \nabla^c_\parallel &= P\nabla_\parallel^f Q \\
    \nabla^{c\dagger}_\parallel &= P \nabla^{f\dagger}_\parallel Q
    \label{eq:sandwich}
\end{align}
where $f$ and $c$ denote fine and coarse grid respectively.
In this way the projection integrals
\begin{align*}
    \int\dV (\nabla_\parallel f) p_i(x)p_j(y)
    \label{}
\end{align*}
are computed more precisely.
The size of the fine grid should therefore be as large as
possible.
We first notice that one interpolation matrix can be absorbed
in the parallel derivative since this also consists of
interpolation operations.
\begin{align}
    \nabla^c_\parallel &= P\nabla_\parallel^{fc} \\
    \nabla^{c\dagger}_\parallel &= \nabla^{fc\dagger}_\parallel Q
    \label{eq:sandwich}
\end{align}
Note that the matrix-matrix multiplications in Eq.~\eqref{eq:sandwich} can
be precomputed and stored. The memory requirements
in the final computations are
therefore the same  as in the old version. (Not entirely, since
the diagonal $v^\varphi/\Delta \varphi$ matrix does not commute with $Q$ or $P$).




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Adjoint operators}
In order to understand what the adjoint operators do let us denote $\Tp$ as the push-forward operator. Then we have
\begin{align}
    \int f(\vec x) \Tp h(\vec x) \sqrt{g(\vec x)}\d^3x% \\
    &=  \int f(\vec x) h(\Tm \vec x)\sqrt{g(\vec x)}\d^3x \nonumber\\
    &=  \int f(\Tp \vec x') h(\vec x')\sqrt{g(\Tp \vec x')}\det J( \Tp\vec x') \d^3x' \nonumber\\
    &=  \int \frac{1}{\sqrt{g(\vec x')}}\Tm\left[\det J(\vec x')\sqrt{g(\vec x')}f(\vec x')\right] h(\vec x')\sqrt{g(\vec x')}   \d^3x' \nonumber \\
    &\equiv  \int (\Tp)^\dagger\left[f(\vec x)\right] h(\vec x)\sqrt{g(\vec x)}   \d^3x
    \label{}
\end{align}
$\det J$ is the determinant of the Jacobian $\partial(\vec x)/\partial(\vec x')$ with $\vec x' = \Tm \vec x$.
In the last step we simply replaced the dummy variable $\vec x'$ with $\vec x$ again and identified the relevant terms
as the adjoint operator:
\begin{align}
    (\Tp)^\dagger f(\vec x ) :=& \frac{1}{\sqrt{g(\vec x)}} \Tm\left[\sqrt{g(\vec x)} \det J(\vec x) f(\vec x) \right]
    = \frac{1}{\sqrt{g(\vec x)}} \Tm\left[\sqrt{G(\vec x)}f(\vec x) \right]
    \nonumber\\
    =& \frac{\sqrt{G(\Tp \vec x)}}{\sqrt{g(\vec x)}} \Tm f(\vec x)
    \label{eq:adjoint_operator}
\end{align}
This means that numerically the adjoint of the push-forward
operator should be a valid discretization of its inverse.
Note that $\Tp [fh] = \Tp f \Tp h$ might not
hold on the discrete level.

Eq.~\eqref{eq:adjoint_operator} can be used to show that \eqref{eq:divBackward}
is indeed the adjoint of \eqref{eq:forward}.

We use \eqref{eq:adjoint_operator} to derive a relation on the discrete level
between the transpose of the interpolation matrices:
\begin{align*}
    \vec f\cdot \sqrt{g} I^{-} \vec h
    =
    \frac{1}{\sqrt{g}} (I^{-})^\mathrm{T} \sqrt{g} \vec f \cdot \sqrt{g}  \vec h
    =
    \frac{\sqrt{G^+}}{\sqrt{g}} I^{+} \vec f \cdot \sqrt{g}  \vec h
\end{align*}
where for the first identity we used the definition of the transpose in linear
algebra and for the second we inserted \eqref{eq:adjoint_operator}.
From this relation we find
\begin{align} \label{eq:unitary}
    \frac{1}{\sqrt{G^+}}(I^{-})^\mathrm{T}\sqrt{g} = I^+
\end{align}
which is in fact a unitary relation between the transpose of an operator and
its inverse.
\subsection{Polar decomposition}
Eq.~\eqref{eq:unitary} does not necessarily hold numerically but may be close.
That is $I^{-}$ might not be exactly unitary in the sense of Eq.~\eqref{eq:unitary}.
In order to find the matrix that is the closest unitary matrix we use an algorithm
by Bj\"orck and Bowie. If $A$ is almost unitary than the closest unitary matrix
is given by the polar decomposition (and derived from the singular value decomposition of $A$):
\begin{align}
    U = A( \Eins - (\Eins - A^{ad} A))^{-1/2} = A \left( \Eins + \frac{1}{2}Q
   +\frac{3}{8}Q^2 + \ldots\right)
\end{align}
where  $Q=\Eins-A^{ad} A$ and the last identity is given by the Taylor expansion of the square root.
In this case the adjoint is defined differently from before
\begin{align}
    A^{ad}  = \frac{1}{\sqrt{G^+}}A^\mathrm{T}\sqrt{g}
\end{align}


%A numerical test could be ( if we neglect the volume form in the adjoint)
%\begin{align}
%    (\Tp)^\dagger \left[J(\vec x)\Tp f(\vec x)\right] - f(\vec x) = 0
%    \label{}
%\end{align}
%Even easier might be
%\begin{align}
%    \left(\Tp\right)^\dagger \det J^{-1}(\vec x ) = 1
%    \label{}
%\end{align}
%
%Finally, let us assume that $\vec v = \bhat$ is the magnetic field
%unit vector. Then we have analytically $\nabla\cdot \vec B= \nabla_\parallel^\dagger B = 0$.
%Numerically, this is true if we have
%\begin{align}
%\left(\Tp\right)^\dagger B^\varphi  = \left(\Tm\right)^\dagger B^\varphi = B^\varphi
%\end{align}
%

\section{Algorithm}
Given are the components $v^i(R,Z)$ for $i\in\{R,Z,\varphi\}$ and a compuational grid (in the following the ``coarse grid``)
\begin{itemize}
  \item generate a fine grid by multiplying the cell numbers of the given coarse grid topologcially (metric and Jacobian of the fine grid are not needed)
  \item integrate the fieldlines for the fine grid:
    \begin{itemize}
      \item evaluate the starting points on the \textbf{coarse} grid in computational space
      \item For a curvilinear grid set up a (higher order, currently 7) grid for the
        interpolation of the vector components $v^i$ and push forward the vector components
        to the curvilinear coordinate system
      \item Integrate the fieldline equations
\begin{subequations}
\begin{align}
\frac{\d \zeta}{\d\varphi} &= \frac{v^\zeta}{v^\varphi}\\
\frac{\d \eta}{\d\varphi} &= \frac{v^\eta}{v^\varphi} \\
\frac{\d s}{\d\varphi} &= \frac{1}{v^\varphi}
\end{align}
\label{eq:fieldlines_converted}
\end{subequations}
    with the given starting points and $s=0$ from $\varphi=0$ until $\varphi = \pm\Delta \varphi$. (Currently we use a Prince-Dormand method with stepsize control for this step).
      \item store the results in $s_{k+1}$ and $s_{k-1}$.
      \item create an interpolation matrix that interpolates from the coarse grid
        to the fine grid
      \item use the interpolation matrix to generate the plus/minus points for the fine grid
    \end{itemize}
  \item create the interpolation matrices that interpolate from the given coarse grid
    to the plus/minus points of the fine grid
  \item create a projection matrix that projects from the fine grid to the coarse grid
  \item compute the matrix-matrix multiplications $P\cdot I^\pm$ as well as their transposes
\end{itemize}

\paragraph{Notes on the MPI implmentation}
It is advantageous to construct $\nabla_\parallel^{fc}$
as s row-distributed matrix with global indices.
This is because a column distributed matrix can be easily (without mpi-communication) multiplied
with a row distributed matrix especially if the indices are global indices.
Each process just multiplies its local matrices.
\begin{align}
M = C\cdot R
\end{align}
This is not true if we started with a column distributed matrix.
The result is then a row distributed matrix with global indices.
From the global indices the gather map/matrix and the local
indices can be constructed.
We note here that we even don't need to construct the gather matrix
for $\nabla_\parallel^{fc}$, only the one for $\nabla_\parallel^c$ is
needed.
\section{Field aligned initialization} \label{sec:parallelc}

An important aspect of our simulations is a judicious initialization of the
fields. We want structures to be field-aligned in the beginning of the simulation with
a possible modulation along the direction of the field line.
If a Gaussian shape is used, we call $\sigma_\parallel$ the extension in parallel
direction and write
\begin{align}
    f_0(R,Z,\varphi) = F(R,Z,\varphi) \exp\left( - \frac{(\varphi-\varphi_0)^2}{2\sigma_\parallel^2}\right),
    \label{eq:parallelInit}
\end{align}
where $F$ is a function that is invariant under the field line transformations
\begin{subequations}
\begin{align}
    \Tp F(\vec z) &= F( \Tp \vec z) \overset{!}{=} F(\vec z) \text{ (pull-back),} \\
    \Tm F(\vec z) &= F( \Tm \vec z) \overset{!}{=} F(\vec z) \text{ (push-forward).}
\end{align}
\label{}
\end{subequations}
We can use these relations to construct aligned structures
by active transformations of some given field.
Our idea is to initialize a two-dimensional field $F(R,Z, \varphi_k)$ in a given plane $k$ and
transform this field to all other planes using the recursive relations
\begin{subequations}
\begin{align}
    F( R, Z, \varphi_{k+1}) = \Tm F( R, Z, \varphi_{k+1}) = F(R^-, Z^-, \varphi_k),\\
    F( R, Z, \varphi_{k-1}) = \Tp F( R, Z, \varphi_{k-1}) = F(R^+, Z^+, \varphi_k),
\end{align}
    \label{eq:recursiveInit}
\end{subequations}
which is the statement that $F$ in the next plane equals the push-forward
and $F$ in the previous plane equals the pull-back of $F$ in the current plane.
Note here that Eq.~\eqref{eq:interpolation} applies for the required interpolation
procedures.


\section{Numerical tests}
The test programs for the parallel derivative are located in
\code{path/to/feltor/inc/geometries}.
To every shared memory test \code{*\_t.cu}
there is a corresponding distributed memory \code{*\_mpit.cu} program.
\code{ds\_t.cu} tests the cylindrical grid
with boundary conditions.
\code{ds\_guenther\_t.cu} tests the cylindrical grid
without boundary conditions i.e. no fieldline leaves the domain.
\code{ds\_curv\_t.cu} tests the implementation
on a flux-aligned grid again with no fieldline leaving the domain.
Finally, \code{ds\_straight\_t.cu} tests the implementation of
completely straight fieldlines and the boundary conditions in the
parallel direction.

The magnetic field in \textsc{Feltor} is given by
\begin{align}
  \vec B = \frac{R_0}{R}( I(\psi_p) \hat e_\varphi + \nabla\psi_p \times\hat e_\varphi)
\end{align}
This gives rise to magnetic field strength and components
\begin{align}
  B = \frac{R_0}{R} \sqrt{ I^2 + \left( \nabla\psi_p \right)^2} \\
  B^R = \frac{R_0}{R}\frac{\partial\psi_p}{\partial Z} \quad
  B^Z = -\frac{R_0}{R}\frac{\partial\psi_p}{\partial R}\quad 
  B^\varphi = \frac{R_0I}{R^2} \\
  \nabla \cdot\bhat = -\nabla_\parallel \ln B = -\frac{R_0}{RB^2} [B, \psi_p]  
  \label{}
\end{align}
where $[.,.]$ is the Poisson bracket. Note that
in order to compute analytical testfunctions we use
\begin{align}
\nabla_\parallel f &= b^R\partial_R f + b^Z\partial_Z f + b^\varphi \partial_\varphi f\\
\Delta_\parallel f &= \nabla\cdot(\bhat\bhat\cdot \nabla f)
= (\nabla\cdot\bhat) \nabla_\parallel f + (\nabla_\parallel b^j) \partial_j f
+ b^ib^j\partial_i \partial_j f
\label{}
\end{align}
with
\begin{align}
\nabla_\parallel b^R &= (\nabla\cdot \bhat) b^R
+ \frac{\psi_Z (\psi_{RZ} - \psi_Z/R) - \psi_{ZZ}\psi_R}{I^2 + (\nabla\psi)^2} \\
\nabla_\parallel b^Z &= (\nabla\cdot \bhat) b^Z
+ \frac{\psi_R (\psi_{RZ} + \psi_Z/R) - \psi_{RR}\psi_Z}{I^2 + (\nabla\psi)^2} \\
\nabla_\parallel b^\varphi &= (\nabla\cdot\bhat) b^\varphi
+ \frac{\psi_Z(I_R/R - 2I/R^2) - I_Z\psi_R/R}{I^2 + (\nabla\psi)^2}
\label{}
\end{align}
where we used $\psi_R \equiv \partial \psi_p /\partial R$ and
$I_R \equiv \partial I(\psi_p) /\partial R$.

\subsection{Cylindrical grid and boundary conditions}
A simple but non-trivial choice for the poloidal flux is
\begin{align}
  \psi_p = \frac{1}{2} \left( (R-R_0)^2 + Z^2 \right) \equiv \frac{1}{2} r^2
  \label{eq:circular}
\end{align}
We choose $R_0 = 10$ and $I=20$ in order to keep the q-factor for the $r=1$ flux surface at $2$.
We set up a domain
$R\in[R_0-1, R_0+1]$,
$Z\in[-1,1]$ and
$\varphi \in [0,2\pi]$ and choose
\begin{align}
    f(R,Z,\varphi) = (\cos(\pi (R-R_0))+1)( \cos(\pi Z /2)+1)\sin(\varphi)
  \label{}
\end{align}
On the chosen domain $f$ respects both Neumann and Dirichlet boundary conditions
both along the field and perpendicular to the wall.
In Tables~\ref{tab:ds_cylindrical_dirichlet1} and
\ref{tab:ds_cylindrical_dirichlet10} we show the convergence of the solution
for various operators and Dirichlet boundary conditions. Note that the same
tables for Neumann boundary conditions exhibit similar values.
Apparently, the discretization for the divergence does not converge even if we
increase the refinement.

\begin{table*}[ht]
\begin{centering}
\footnotesize
\hspace*{-2cm}
\input{ds_cylindrical_dirichlet1.tex}
\caption{Convergence Table for Dirichlet boundary conditions and $m=1$,
$N_R=N_Z=N$. Centered discretization with boundary conditions along the fieldline. The table for Neumann conditions exhibits similar numbers.}
\label{tab:ds_cylindrical_dirichlet1}
\end{centering}
\end{table*}

\begin{table*}[ht]
\begin{centering}
\footnotesize
\hspace*{-2cm}
\input{ds_cylindrical_dirichlet10.tex}
\caption{Convergence Table for Dirichlet boundary conditions and $m=10$,
$N_R=N_Z=N$. Centered discretization with boundary conditions along the fieldline. The table for Neumann conditions exhibits similar numbers.}
\label{tab:ds_cylindrical_dirichlet10}
\end{centering}
\end{table*}

\subsection{The G\"unther field}
A simple choice for the flux function that makes fieldlines stay
inside a square box is
\begin{align}
  \psi_p = \cos(\pi (R-R_0)/2) \cos(\pi Z /2)
\label{}
\end{align}
Again, we set up a domain
$R\in[R_0-1, R_0+1]$,
$Z\in[-1,1]$ and
$\varphi \in [0,2\pi]$ and choose
\begin{align}
  f_1(R,Z,\varphi) = -\psi_p(R,Z)\cos(\varphi)\\
  f_2(R,Z,\varphi) = -\psi_p(R,Z)\cos(\varphi) + (R-R_0)^2/4 + Z(R-R_0)/4
  \label{}
\end{align}
\begin{table*}[ht]
\begin{centering}
\footnotesize
\hspace*{-2cm}
\input{ds_guenther10.tex}
\caption{Convergence Table for $m=10$ and $N_R=N_Z=N$ and the G\"unther field. Centered discretizations, no boundary condition.
}
\label{tab:ds_guenther10}
\end{centering}
\end{table*}
\subsection{Curvilinear grid}
Here we choose the general Solov'ev equilibrium for
$\psi_p$ and choose to use $f_1$ or $f_2$ again.
We use the simple orthogonal flux to construct
grids on the ring bounded by $\psi_p=-20$ and $\psi_p=-4$.
\begin{table*}[ht]
\begin{centering}
\footnotesize
\hspace*{-2cm}
\input{ds_curv1000.tex}
\caption{Convergence Table for $m=1000$, $N_R = 2$, $N_Z=N$ on a curvilinear grid. Centered discretizations, no boundary condition.
}
\label{tab:ds_curv1000}
\end{centering}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Performance considerations}
The performance of the main matrix-vector multiplication depends, as do all our
routines, on the number of memory loads and stores per vector element.
A dG interpolation matrix in two dimensions has $n^2$ points per line since
it uses all $n^2$ points in one dG-cell to compute the result.
A matrix-vector multiplication with the interpolation matrix thus needs $n^2+3$
vector loads and stores, with the $3$ coming from loading the input vector and
loading and storing the output vector.
However, since we use the refinement-projection aproach to our discretization
the number of points per line of the interpolation matrix increases to typically
$4n^2$ since now also neighboring cells are used for the computation.

For the discretization of $\nabla_\parallel$ this in total means that the number
of memory operations per element is
\begin{align}
m &= 2\cdot(4n^2+3)+5\quad \text{ (refined)} \\
m &= 2\cdot(n^2+3)+5\quad \text{ (unrefined)}
\end{align}
with $n$ the number of polynomial coefficients.
This is comparable to one iteration of a CG method in three dimensions.





%..................................................................
\bibliography{../references}
%..................................................................


\end{document}

